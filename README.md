# Hindi BabyLM: Data-Efficient Language Modeling for Hindi

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

A comprehensive implementation of data-efficient language modeling for Hindi, designed as a BabyLM challenge adaptation for morphologically rich languages. This project trains transformer-based language models with developmentally plausible amounts of data (~10M tokens) and includes extensive evaluation frameworks for linguistic competence assessment.

## ğŸŒŸ Key Features

- **Enhanced GPT Architecture**: 3 model sizes (Tiny: 50M, Small: 110M, Medium: 350M parameters)
- **Curriculum Learning**: 5 training strategies Ã— 5 scheduling approaches (25 combinations)
- **Advanced Training Pipeline**: Multiple optimizers (AdamW, Adam, SGD), LR schedulers, mixed precision (FP16/BF16)
- **Comprehensive Tokenization**: SentencePiece, WordPiece, BPE with morphological preservation analysis
- **MultiBLiMP Evaluation**: 14 Hindi linguistic phenomena with 70+ minimal pairs
- **Morphological Probes**: 10 probe tasks for layer-wise linguistic feature analysis
- **Statistical Analysis**: Paired t-tests, Wilcoxon tests, effect sizes, bootstrap confidence intervals
- **Publication-Ready Figures**: 10+ plot types using ThesisPlotter with consistent styling
- **LaTeX Integration**: Automatic generation of thesis-ready tables and figures
- **Interactive Notebooks**: 2 comprehensive Jupyter notebooks for data exploration and results analysis
- **Experiment Tracking**: Automatic logging with Weights & Biases integration
- **IndicCorp V2 Integration**: Automated download from AI4Bharat/HuggingFace with streaming support
- **Multi-Source Corpus**: IndicCorp, Hindi Wikipedia, children's literature
- **Advanced Quality Filtering**: Length-based, language detection (Devanagari ratio), deduplication (MinHash LSH)
- **Intelligent Data Mixing**: Configurable source ratios with token-level precision

## ğŸ“ Project Structure

```
hindi-babylm/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Raw downloaded datasets
â”‚   â”‚   â”œâ”€â”€ indiccorp_hindi.txt       # IndicCorp Hindi corpus
â”‚   â”‚   â”œâ”€â”€ indiccorp_hindi.pkl       # Pickled format
â”‚   â”‚   â”œâ”€â”€ indiccorp_metadata.json   # Dataset metadata
â”‚   â”‚   â””â”€â”€ raw_corpus.pkl            # Combined raw data
â”‚   â”œâ”€â”€ splits/                       # Train/val/test splits (NEW PATH)
â”‚   â”‚   â”œâ”€â”€ train.pkl / train.txt     # Training data
â”‚   â”‚   â”œâ”€â”€ val.pkl / val.txt         # Validation data
â”‚   â”‚   â”œâ”€â”€ test.pkl / test.txt       # Test data
â”‚   â”‚   â””â”€â”€ metadata.json             # Split metadata
â”‚   â””â”€â”€ tokenized/                    # Tokenized datasets
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â”œâ”€â”€ indiccorp_downloader.py   # IndicCorp V2 downloader (IMPLEMENTED)
â”‚   â”‚   â”œâ”€â”€ wiki_scraper.py           # Wikipedia scraper
â”‚   â”‚   â”œâ”€â”€ childrens_books.py        # Children's literature
â”‚   â”‚   â”œâ”€â”€ corpus_builder.py         # Main corpus orchestration
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py           # Unicode normalization, cleaning
â”‚   â”‚   â”œâ”€â”€ quality_filter.py         # Length/language filtering
â”‚   â”‚   â”œâ”€â”€ deduplicator.py           # MinHash LSH deduplication
â”‚   â”‚   â”œâ”€â”€ data_mixer.py             # Multi-source data mixing
â”‚   â”‚   â””â”€â”€ corpus_analyzer.py        # Statistics generation
â”‚   â”‚
â”‚   â”œâ”€â”€ tokenization/
â”‚   â”‚   â”œâ”€â”€ tokenizer_factory.py      # Tokenizer creation hub
â”‚   â”‚   â”œâ”€â”€ sentencepiece_trainer.py  # SentencePiece training
â”‚   â”‚   â”œâ”€â”€ wordpiece_trainer.py      # WordPiece training
â”‚   â”‚   â”œâ”€â”€ bpe_trainer.py            # BPE training
â”‚   â”‚   â””â”€â”€ morphological_analyzer.py # Morphological preservation metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ model_factory.py          # Model creation hub
â”‚   â”‚   â”œâ”€â”€ enhanced_gpt.py           # Enhanced GPT (50M/110M/350M)
â”‚   â”‚   â””â”€â”€ bert_model.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py                # Enhanced trainer with curriculum learning
â”‚   â”‚   â”œâ”€â”€ curriculum_learning.py    # 5 curriculum strategies
â”‚   â”‚   â”œâ”€â”€ curriculum_schedule.py    # 5 scheduling approaches
â”‚   â”‚   â”œâ”€â”€ optimizer_factory.py      # Multiple optimizer support
â”‚   â”‚   â”œâ”€â”€ scheduler_factory.py      # LR scheduling strategies
â”‚   â”‚   â””â”€â”€ mixed_precision.py        # FP16/BF16 training
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluation_manager.py     # Evaluation orchestration
â”‚   â”‚   â”œâ”€â”€ indicglue_evaluator.py    # Hindi NLP benchmark
â”‚   â”‚   â”œâ”€â”€ multiblimp_evaluator.py   # 14 linguistic phenomena
â”‚   â”‚   â”œâ”€â”€ morphological_probes.py   # 10 morphological probe tasks
â”‚   â”‚   â””â”€â”€ perplexity_evaluator.py   # Language modeling metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                     # Phase 2: Analysis & Visualization
â”‚   â”‚   â”œâ”€â”€ results_analyzer.py       # Statistical testing & reporting
â”‚   â”‚   â””â”€â”€ visualization_utils.py    # ThesisPlotter - publication figures
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ experiment_config.py      # Configuration management
â”‚       â”œâ”€â”€ seed_manager.py           # Reproducibility utilities
â”‚       â”œâ”€â”€ checkpoint_manager.py     # Model checkpoint handling
â”‚       â””â”€â”€ logger.py                 # Logging utilities
â”‚
â”œâ”€â”€ notebooks/                        # Interactive Analysis
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb     # Corpus statistics & quality
â”‚   â””â”€â”€ 02_results_analysis.ipynb     # Experiment results & visualizations
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_experiment.py             # Main experiment orchestrator
â”‚   â”œâ”€â”€ run_tokenization_experiments.py
â”‚   â”œâ”€â”€ run_architecture_experiments.py
â”‚   â””â”€â”€ run_curriculum_experiments.py
â”‚
â”œâ”€â”€ configs/                          # Configuration Files
â”‚   â”œâ”€â”€ base_config.yaml              # Base configuration
â”‚   â”œâ”€â”€ tiny_model.yaml               # 50M parameter model
â”‚   â”œâ”€â”€ small_model.yaml              # 110M parameter model
â”‚   â”œâ”€â”€ medium_model.yaml             # 350M parameter model
â”‚   â”œâ”€â”€ curriculum_learning.yaml      # Curriculum learning configs
â”‚   â””â”€â”€ position_encodings.yaml       # Position encoding variants
â”‚
â”œâ”€â”€ docs/                             # Comprehensive Documentation
â”‚   â”œâ”€â”€ 01_PROJECT_OVERVIEW.md        # Project architecture & phases
â”‚   â”œâ”€â”€ 02_DATA_PROCESSING.md         # Data pipeline & IndicCorp
â”‚   â”œâ”€â”€ 03_TOKENIZATION.md            # Tokenization strategies
â”‚   â”œâ”€â”€ 04_MODELS.md                  # Model architectures & position encodings
â”‚   â”œâ”€â”€ 05_TRAINING.md                # Training & curriculum learning
â”‚   â”œâ”€â”€ 06_EVALUATION.md              # Evaluation frameworks
â”‚   â”œâ”€â”€ 07_CONFIGURATION.md           # Configuration guide
â”‚   â”œâ”€â”€ 08_ANALYSIS_AND_VISUALIZATION.md  # ResultsAnalyzer & ThesisPlotter
â”‚   â”œâ”€â”€ 09_JUPYTER_NOTEBOOKS.md       # Notebook usage guide
â”‚   â””â”€â”€ 10_THESIS_INTEGRATION.md      # LaTeX integration & thesis workflow
â”‚
â”œâ”€â”€ results/                          # Experiment Results
â”‚   â””â”€â”€ [experiment_name]/
â”‚       â”œâ”€â”€ metadata.json
â”‚       â”œâ”€â”€ config.yaml
â”‚       â”œâ”€â”€ training_summary.json
â”‚       â”œâ”€â”€ evaluation_results.json
â”‚       â””â”€â”€ checkpoints/
â”‚
â”œâ”€â”€ figures/                          # Generated Figures
â”‚   â”œâ”€â”€ training_curves.png
â”‚   â”œâ”€â”€ multiblimp_comparison.png
â”‚   â”œâ”€â”€ morphological_probes_comparison.png
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tables/                           # LaTeX Tables
â”‚   â”œâ”€â”€ indicglue_results.tex
â”‚   â”œâ”€â”€ multiblimp_results.tex
â”‚   â””â”€â”€ probes_results.tex
â”‚
â”œâ”€â”€ reports/                          # Markdown Reports
â”‚   â””â”€â”€ [experiment_name]_report.md
â”‚
â”œâ”€â”€ slurm_scripts/                    # HPC/LRZ Job Scripts
â”‚   â”œâ”€â”€ README_LRZ.md                 # Complete LRZ setup guide
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md            # Command cheatsheet
â”‚   â”œâ”€â”€ run_complete_pipeline.sh      # Full pipeline (24h, 1 GPU)
â”‚   â”œâ”€â”€ run_data_processing.sh        # Data only (4h, CPU)
â”‚   â”œâ”€â”€ run_training.sh               # Training only (48h, 1 GPU)
â”‚   â”œâ”€â”€ run_evaluation.sh             # Evaluation only (8h, 1 GPU)
â”‚   â”œâ”€â”€ run_tiny_model.sh             # Quick test (12h, 1 GPU)
â”‚   â””â”€â”€ run_curriculum_learning.sh    # Curriculum learning (48h, 1 GPU)
â”‚
â””â”€â”€ logs/                             # SLURM job logs (created automatically)

```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- CUDA (optional, for GPU training)
- 16GB+ RAM recommended
- 50GB+ disk space for data and models

### Running on HPC Systems (LRZ, etc.)

**For LRZ users**, we provide ready-to-use SLURM scripts in `slurm_scripts/`. See the complete [LRZ Setup Guide](slurm_scripts/README_LRZ.md) for detailed instructions.

**Quick Start on LRZ:**

```bash
# 1. Setup (one-time)
module load python/3.10
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
mkdir -p logs

# 2. Update email in scripts
sed -i 's/your.email@tum.de/YOUR_EMAIL@tum.de/g' slurm_scripts/*.sh

# 3. Submit complete pipeline job
sbatch slurm_scripts/run_complete_pipeline.sh

# 4. Monitor job
squeue -u $USER
tail -f logs/pipeline_*.out
```

Available SLURM scripts:
- `run_complete_pipeline.sh` - Full pipeline (24h, 1 GPU)
- `run_data_processing.sh` - Data only (4h, CPU)
- `run_training.sh` - Training only (48h, 1 GPU)
- `run_evaluation.sh` - Evaluation only (8h, 1 GPU)
- `run_tiny_model.sh` - Quick test (12h, 1 GPU)
- `run_curriculum_learning.sh` - Curriculum learning (48h, 1 GPU)

**Key Features:**
- âœ… GPU auto-detection and setup
- âœ… Module loading (Python, CUDA, cuDNN)
- âœ… Automatic logging to `logs/` directory
- âœ… Email notifications on completion
- âœ… Resource optimization for LRZ partitions
- âœ… Checkpoint resumption support

See [slurm_scripts/README_LRZ.md](slurm_scripts/README_LRZ.md) for troubleshooting and advanced usage.

### Installation (Local Machine)

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/hindi-babylm.git
cd hindi-babylm
```

2. **Set up environment (automated)**
```bash
chmod +x setup_env.sh
./setup_env.sh
```

Or manually:
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. **Install project in development mode**
```bash
pip install -e .
```

### Basic Usage

#### 1. Complete Pipeline (Recommended for First Run)

```bash
# Run complete pipeline: data â†’ training â†’ evaluation
python main.py \
    --config configs/base_config.yaml \
    --experiment_name my_first_experiment
```

This single command will:
- Download and process data from all sources (IndicCorp, Wikipedia, children's books)
- Create train/val/test splits in `data/splits/`
- Train a language model with your configuration
- Run comprehensive evaluation (IndicGLUE, MultiBLiMP, Morphological Probes)
- Save all results to `results/my_first_experiment/`

#### 2. Stage-by-Stage Execution

**Data Processing Only:**
```bash
# Download IndicCorp Hindi (100K samples â‰ˆ 6M tokens) - standalone
python src/data_processing/indiccorp_downloader.py \
    --output-dir data/raw \
    --num-samples 100000 \
    --format both

# Or build complete corpus with all sources via main pipeline
python main.py \
    --config configs/base_config.yaml \
    --stage data \
    --experiment_name data_processing_only
```

**Training Only** (requires existing data):
```bash
# Train Tiny model (50M params) - good for testing
python main.py \
    --config configs/tiny_model.yaml \
    --stage train \
    --experiment_name tiny_baseline

# Train Small model (110M params) with curriculum learning
python main.py \
    --config configs/curriculum_learning.yaml \
    --stage train \
    --experiment_name small_curriculum
```

**Evaluation Only** (requires trained model):
```bash
# Evaluate on all benchmarks (IndicGLUE, MultiBLiMP, Probes)
python main.py \
    --config configs/base_config.yaml \
    --stage eval \
    --experiment_name tiny_baseline
```

#### 3. Advanced Options

**Resume Training from Checkpoint:**
```bash
python main.py \
    --config configs/base_config.yaml \
    --stage train \
    --experiment_name resumed_training \
    --resume results/previous_exp/checkpoints/checkpoint_epoch_5.pt
```

**Force Reprocess Data:**
```bash
# Useful when you've updated data sources or filtering parameters
python main.py \
    --config configs/base_config.yaml \
    --stage all \
    --experiment_name fresh_run \
    --force-reprocess
```

**Custom Random Seed:**
```bash
# Override config seed for reproducibility experiments
python main.py \
    --config configs/base_config.yaml \
    --experiment_name seed_experiment \
    --seed 42
```

**Specify Device:**
```bash
# Force CPU usage (useful for debugging)
python main.py \
    --config configs/base_config.yaml \
    --experiment_name cpu_run \
    --device cpu

# Force GPU usage
python main.py \
    --config configs/base_config.yaml \
    --experiment_name gpu_run \
    --device cuda
```

### Advanced Usage

#### Experiment with Position Encodings

```bash
# Train with RoPE (Rotary Position Embeddings)
python main.py \
    --config configs/position_encodings.yaml \
    --experiment_name rope_experiment

# Train with ALiBi (Attention with Linear Biases)
python main.py \
    --config configs/position_encodings.yaml \
    --experiment_name alibi_experiment

# Compare all 5 position encoding types (automated suite)
python experiments/run_architecture_experiments.py
```

#### Curriculum Learning Experiments

```bash
# Specific curriculum strategy with morphological ordering
python main.py \
    --config configs/curriculum_learning.yaml \
    --experiment_name morphological_curriculum

# Length-based curriculum learning
python main.py \
    --config configs/curriculum_learning.yaml \
    --experiment_name length_curriculum

# Run all 25 curriculum learning configurations (automated suite)
python experiments/run_curriculum_experiments.py
```

#### Tokenization Comparison

```bash
# Train with SentencePiece tokenization
python main.py \
    --config configs/sentencepiece_config.yaml \
    --experiment_name sentencepiece_exp

# Compare SentencePiece, WordPiece, BPE (automated suite)
python experiments/run_tokenization_experiments.py
```

#### Model Size Experiments

```bash
# Tiny model (50M parameters) - fast training
python main.py \
    --config configs/tiny_model.yaml \
    --experiment_name tiny_50m

# Small model (110M parameters) - balanced
python main.py \
    --config configs/small_model.yaml \
    --experiment_name small_110m

# Medium model (350M parameters) - best performance
python main.py \
    --config configs/medium_model.yaml \
    --experiment_name medium_350m
```

## ğŸ“Š Data Processing Pipeline

### 1. IndicCorp V2 Download

The project includes a fully implemented IndicCorp downloader:

```python
from src.data_processing.indiccorp_downloader import download_indiccorp_hindi

# Simple download
paths = download_indiccorp_hindi(
    output_dir='data/raw',
    num_samples=100000,
    streaming=False,
    save_format='both'
)

# Returns:
# {
#     'text': Path('data/raw/indiccorp_hindi.txt'),
#     'pickle': Path('data/raw/indiccorp_hindi.pkl'),
#     'statistics': Path('data/raw/indiccorp_statistics.json'),
#     'metadata': Path('data/raw/indiccorp_metadata.json')
# }
```

**Features**:
- HuggingFace integration with automatic caching
- Streaming mode for memory efficiency
- Comprehensive statistics and metadata
- Command-line interface
- Progress tracking with tqdm

### 2. Quality Filtering

- **Length Filtering**: Remove too short (<10 chars) or too long (>1000 chars) texts
- **Language Detection**: Ensure â‰¥80% Devanagari characters
- **Deduplication**: MinHash LSH algorithm for exact and near-duplicate detection
- **Token Limiting**: Precisely limit corpus to ~10M tokens

### 3. Data Splits

Splits are saved to `data/splits/` (updated path):
- **Train**: 80% (~8M tokens)
- **Validation**: 10% (~1M tokens)
- **Test**: 10% (~1M tokens)

## ğŸ”§ Configuration

All experiments are configured via YAML files in `configs/`. Key configuration sections:

```yaml
# configs/base_config.yaml

data:
  sources:
    indiccorp: 0.6      # 60% from IndicCorp
    wikipedia: 0.3      # 30% from Wikipedia
    childrens_books: 0.1  # 10% from children's books
  max_tokens: 10_000_000

tokenization:
  type: sentencepiece    # sentencepiece, wordpiece, bpe
  vocab_size: 32000
  model_type: unigram    # unigram, bpe, char, word

model:
  type: enhanced_gpt
  size: small            # tiny (50M), small (110M), medium (350M)
  position_encoding: sinusoidal  # sinusoidal, learned, rope, alibi, relative_bias
  architecture:
    hidden_size: 768
    num_layers: 12
    num_heads: 12
    max_position_embeddings: 1024

training:
  batch_size: 32
  num_epochs: 10
  learning_rate: 5e-4
  optimizer: adamw       # adamw, adam, sgd
  scheduler: cosine      # linear, cosine, constant
  mixed_precision: fp16  # fp16, bf16, fp32

  curriculum:
    use_curriculum: true
    curriculum_strategy: morphological  # morphological, length, frequency, combined, dynamic
    curriculum_schedule: linear         # linear, root, exponential, step, performance_based

evaluation:
  benchmarks:
    - indicglue          # Hindi NLP tasks
    - multiblimp         # 14 linguistic phenomena
    - morphological_probes  # 10 probe tasks

reproducibility:
  seed: 42
  set_deterministic: true
```

See [Configuration Documentation](docs/07_CONFIGURATION.md) for complete guide.

## ğŸ“ˆ Evaluation Frameworks

### 1. MultiBLiMP (14 Linguistic Phenomena)

Tests grammatical competence with minimal pairs:

**Agreement**:
- `subject_verb_agreement_number`: à¤¸à¤‚à¤–à¥à¤¯à¤¾ à¤¸à¤¹à¤®à¤¤à¤¿ (number)
- `subject_verb_agreement_person`: à¤ªà¥à¤°à¥à¤· à¤¸à¤¹à¤®à¤¤à¤¿ (person)
- `subject_verb_agreement_gender`: à¤²à¤¿à¤‚à¤— à¤¸à¤¹à¤®à¤¤à¤¿ (gender)
- `determiner_noun_agreement`: à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤•-à¤¸à¤‚à¤œà¥à¤à¤¾ à¤¸à¤¹à¤®à¤¤à¤¿

**Case Marking**:
- `ergative_case`: à¤•à¤°à¥à¤¤à¥ƒà¤•à¤¾à¤°à¤• (-à¤¨à¥‡)
- `accusative_case`: à¤•à¤°à¥à¤®à¤•à¤¾à¤°à¤• (-à¤•à¥‹)
- `instrumental_case`: à¤•à¤°à¤£à¤•à¤¾à¤°à¤• (-à¤¸à¥‡)

**And 7 more phenomena** (word order, scrambling, binding, etc.)

### 2. Morphological Probes (10 Tasks)

Layer-wise analysis of morphological features:
- Number detection, Gender detection, Person detection
- Case marking, Tense/aspect, Mood detection
- Voice detection, Definiteness, Postposition attachment
- Compound verb structure

### 3. IndicGLUE

Standard Hindi NLP benchmarks for downstream task evaluation.

## ğŸ“Š Results Analysis

### Interactive Jupyter Notebooks

#### 1. Data Exploration (`notebooks/01_data_exploration.ipynb`)

Analyzes corpus characteristics:
- Basic statistics (tokens, TTR, sentence lengths)
- Length distributions with histograms
- Character analysis (Devanagari ratio, frequency)
- Word frequency analysis
- Morphological complexity (case markers: à¤¨à¥‡, à¤•à¥‹, à¤¸à¥‡, à¤•à¤¾, etc.)
- Data quality assessment

**Generated Outputs**:
- `figures/length_distributions.png`
- `figures/character_distribution.png`
- `figures/word_frequency.png`
- `figures/case_markers.png`
- `data/corpus_statistics.json`

#### 2. Results Analysis (`notebooks/02_results_analysis.ipynb`)

Comprehensive experiment analysis:
- Training curves comparison
- IndicGLUE/MultiBLiMP performance comparison
- Statistical significance testing (t-test, Wilcoxon, effect sizes)
- Layer-wise probe visualizations
- LaTeX table generation for thesis

**Generated Outputs**:
- `figures/training_curves.png`
- `figures/multiblimp_comparison.png`
- `figures/morphological_probes_comparison.png`
- `tables/indicglue_results.tex`
- `tables/multiblimp_results.tex`
- `reports/[experiment]_report.md`

### Command-Line Analysis

```bash
# Generate comprehensive analysis for all experiments
python src/analysis/results_analyzer.py \
    --results_dir results/ \
    --output_dir analysis/

# Compare two specific experiments with statistical tests
python src/analysis/results_analyzer.py \
    --compare baseline_exp curriculum_exp \
    --metric accuracy \
    --alpha 0.05
```

### Statistical Testing

The `ResultsAnalyzer` automatically performs:
- **Paired t-test**: Parametric significance testing
- **Wilcoxon signed-rank test**: Non-parametric alternative
- **Cohen's d**: Effect size calculation
- **Bootstrap confidence intervals**: 95% CI with 10,000 iterations

## ğŸ“ Thesis Integration

This project is designed for academic thesis work with first-class LaTeX support.

### Automatic LaTeX Table Generation

```python
from src.analysis.results_analyzer import analyze_experiments

analyzer = analyze_experiments(results_dir='results/')

# Generate thesis-ready LaTeX table
latex_table = analyzer.generate_latex_table(
    eval_type='multiblimp',
    metric='accuracy',
    caption='MultiBLiMP Syntactic Evaluation Results',
    label='tab:multiblimp',
    save_path='tables/multiblimp_results.tex'
)

# In your thesis .tex file:
# \input{tables/multiblimp_results.tex}
```

### Publication-Quality Figures

```python
from src.analysis.visualization_utils import ThesisPlotter

plotter = ThesisPlotter(style='thesis')  # Consistent IEEE/thesis styling

# Training curves with confidence intervals
fig = plotter.plot_training_curves_with_ci(
    experiments=['baseline', 'curriculum'],
    metrics=['loss', 'perplexity'],
    save_path='figures/training_comparison.png'
)
```

See [Thesis Integration Guide](docs/10_THESIS_INTEGRATION.md) for complete workflow.

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` directory:

| Document | Description |
|----------|-------------|
| [01_PROJECT_OVERVIEW.md](docs/01_PROJECT_OVERVIEW.md) | Project architecture, phases, and statistics |
| [02_DATA_PROCESSING.md](docs/02_DATA_PROCESSING.md) | Data pipeline, IndicCorp downloader, quality filtering |
| [03_TOKENIZATION.md](docs/03_TOKENIZATION.md) | Tokenization strategies and morphological analysis |
| [04_MODELS.md](docs/04_MODELS.md) | Model architectures and 5 position encoding types |
| [05_TRAINING.md](docs/05_TRAINING.md) | Training pipeline and curriculum learning (5Ã—5 matrix) |
| [06_EVALUATION.md](docs/06_EVALUATION.md) | MultiBLiMP (14 phenomena), morphological probes (10 tasks) |
| [07_CONFIGURATION.md](docs/07_CONFIGURATION.md) | Complete configuration reference |
| [08_ANALYSIS_AND_VISUALIZATION.md](docs/08_ANALYSIS_AND_VISUALIZATION.md) | ResultsAnalyzer and ThesisPlotter API |
| [09_JUPYTER_NOTEBOOKS.md](docs/09_JUPYTER_NOTEBOOKS.md) | Interactive analysis workflows |
| [10_THESIS_INTEGRATION.md](docs/10_THESIS_INTEGRATION.md) | LaTeX integration and thesis workflow |

**Documentation Statistics**: ~55,000 lines covering all aspects of the project.

## ğŸ§ª Running Experiments

### Complete Experiment Suite

```bash
# 1. Tokenization comparison (SentencePiece vs WordPiece vs BPE)
python experiments/run_tokenization_experiments.py

# 2. Position encoding comparison (5 types)
python experiments/run_architecture_experiments.py

# 3. Curriculum learning comparison (5 strategies Ã— 5 schedules)
python experiments/run_curriculum_experiments.py

# 4. Model size comparison (Tiny 50M, Small 110M, Medium 350M)
python experiments/run_model_size_experiments.py
```

### Custom Experiment with main.py

```bash
# Complete pipeline with custom configuration
python main.py \
    --config configs/my_custom_config.yaml \
    --experiment_name my_custom_experiment

# With all advanced options
python main.py \
    --config configs/my_custom_config.yaml \
    --experiment_name my_experiment \
    --seed 42 \
    --device cuda \
    --force-reprocess
```

### Programmatic Usage (Advanced)

For more complex orchestration, use the ExperimentOrchestrator:

```python
from experiments.run_experiment import ExperimentOrchestrator

# Initialize with custom config
orchestrator = ExperimentOrchestrator(
    config_path='configs/my_custom_config.yaml',
    experiment_name='my_experiment'
)

# Run complete pipeline
result = orchestrator.run_full_pipeline()

# Or run specific stages with more control
splits = orchestrator.run_data_processing()
model, tokenizer = orchestrator.run_training(splits)
results = orchestrator.run_evaluation(model, tokenizer, splits)
```

### Resume Training

```bash
# Resume from checkpoint using main.py
python main.py \
    --config configs/base_config.yaml \
    --stage train \
    --experiment_name resumed_training \
    --resume results/previous_exp/checkpoints/checkpoint_epoch_5.pt

# Resume with different config (transfer learning)
python main.py \
    --config configs/fine_tuning_config.yaml \
    --stage train \
    --experiment_name fine_tuned \
    --resume results/base_model/checkpoints/checkpoint_best.pt
```

## ğŸ”¬ Key Research Questions

This implementation explores:

1. **Position Encodings**: Which position encoding (RoPE, ALiBi, etc.) works best for Hindi's morphologically rich structure?

2. **Curriculum Learning**: Does morphology-based curriculum learning improve syntactic competence more than length-based or frequency-based curricula?

3. **Data Efficiency**: Can models learn Hindi grammar with only 10M tokens (vs billions in typical pretraining)?

4. **Tokenization**: How do different tokenization strategies (SentencePiece, WordPiece, BPE) preserve Hindi morphological boundaries?

5. **Model Size**: What's the optimal model size for data-efficient Hindi language modeling?

## ğŸ“¦ Project Statistics

- **Total Lines of Code**: ~12,500 lines
- **Documentation**: ~55,000 lines
- **Python Modules**: 45+ files
- **Configuration Templates**: 5 YAML files
- **Evaluation Tasks**: 24 total (14 MultiBLiMP + 10 Probes)
- **Supported Position Encodings**: 5 types
- **Curriculum Strategies**: 5 strategies Ã— 5 schedules = 25 combinations
- **Model Sizes**: 3 (Tiny 50M, Small 110M, Medium 350M)

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **AI4Bharat** for the IndicCorp V2 dataset
- **BabyLM Challenge organizers** for inspiration
- **Technical University of Munich** for supporting this research
- **PyTorch** and **HuggingFace** teams for excellent frameworks

## ğŸ“§ Contact

**Ayush Kumar**
Technical University of Munich
Email: ayush.kumar@tum.de
GitHub: [@ayushtalreja](https://github.com/ayushtalreja)

## ğŸ“– Citation

If you use this code or data for research, please cite:

```bibtex
@mastersthesis{kumar2025hindi_babylm,
  title={Hindi BabyLM: Data-Efficient Language Modeling for Morphologically Rich Languages},
  author={Kumar, Ayush},
  year={2025},
  school={Technical University of Munich},
  type={Master's Thesis},
  note={Implementation includes 5 position encodings, curriculum learning (25 configurations),
        MultiBLiMP evaluation (14 phenomena), and morphological probes (10 tasks)}
}
```

## ğŸ—ºï¸ Roadmap

- [x] Phase 1: Core Implementation (Models, Training, Evaluation)
- [x] Phase 2: Analysis & Visualization (Statistical testing, LaTeX integration)
- [ ] Phase 3: Extended Experiments (Cross-lingual transfer, multilingual models)
- [ ] Phase 4: Deployment (Model serving, API endpoints)

---

**Built with â¤ï¸ for Hindi NLP research**
