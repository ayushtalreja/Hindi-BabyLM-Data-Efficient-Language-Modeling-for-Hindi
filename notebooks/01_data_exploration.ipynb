{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for Hindi BabyLM\n",
    "\n",
    "This notebook provides comprehensive analysis of the Hindi training corpus:\n",
    "- Corpus statistics\n",
    "- Length distributions\n",
    "- Character analysis\n",
    "- Morphological complexity\n",
    "- Data quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path for cluster\n",
    "current = Path.cwd()\n",
    "if current.name == 'workspace' or 'workspace' in str(current):\n",
    "    project_root = current.parent / 'dss' / 'dsshome1' / '00' / 'ge95xod2' / 'Hindi-BabyLM-Data-Efficient-Language-Modeling-for-Hindi'\n",
    "else:\n",
    "    project_root = current.parent if current.name == 'notebooks' else current\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Project imports\n",
    "from src.data_processing.text_cleaner import clean_text\n",
    "from src.data_processing.quality_filter import QualityFilter\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus files\n",
    "data_dir = Path('../data/splits')\n",
    "\n",
    "# Load train/val/test splits\n",
    "with open(data_dir / 'train.txt', 'r', encoding='utf-8') as f:\n",
    "    train_texts = f.readlines()\n",
    "\n",
    "with open(data_dir / 'val.txt', 'r', encoding='utf-8') as f:\n",
    "    val_texts = f.readlines()\n",
    "\n",
    "with open(data_dir / 'test.txt', 'r', encoding='utf-8') as f:\n",
    "    test_texts = f.readlines()\n",
    "\n",
    "print(f\"Training examples: {len(train_texts):,}\")\n",
    "print(f\"Validation examples: {len(val_texts):,}\")\n",
    "print(f\"Test examples: {len(test_texts):,}\")\n",
    "print(f\"Total examples: {len(train_texts) + len(val_texts) + len(test_texts):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(texts):\n",
    "    \"\"\"Compute corpus statistics\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Character counts\n",
    "    all_text = ''.join(texts)\n",
    "    stats['total_characters'] = len(all_text)\n",
    "    \n",
    "    # Word counts\n",
    "    all_words = [word for text in texts for word in text.split()]\n",
    "    stats['total_words'] = len(all_words)\n",
    "    stats['unique_words'] = len(set(all_words))\n",
    "    \n",
    "    # Sentence lengths\n",
    "    word_counts = [len(text.split()) for text in texts]\n",
    "    stats['avg_sentence_length'] = np.mean(word_counts)\n",
    "    stats['median_sentence_length'] = np.median(word_counts)\n",
    "    stats['std_sentence_length'] = np.std(word_counts)\n",
    "    \n",
    "    # Character lengths\n",
    "    char_counts = [len(text) for text in texts]\n",
    "    stats['avg_char_length'] = np.mean(char_counts)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "train_stats = compute_statistics(train_texts)\n",
    "\n",
    "print(\"ðŸ“Š Corpus Statistics:\")\n",
    "print(f\"  Total tokens: {train_stats['total_words']:,}\")\n",
    "print(f\"  Unique tokens: {train_stats['unique_words']:,}\")\n",
    "print(f\"  Type-Token Ratio: {train_stats['unique_words']/train_stats['total_words']:.4f}\")\n",
    "print(f\"  Avg sentence length: {train_stats['avg_sentence_length']:.2f} words\")\n",
    "print(f\"  Median sentence length: {train_stats['median_sentence_length']:.0f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Length Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute length distributions\n",
    "train_word_counts = [len(text.split()) for text in train_texts]\n",
    "train_char_counts = [len(text) for text in train_texts]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Word count distribution\n",
    "ax1.hist(train_word_counts, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(np.mean(train_word_counts), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(train_word_counts):.1f}')\n",
    "ax1.axvline(np.median(train_word_counts), color='green', linestyle='--',\n",
    "            label=f'Median: {np.median(train_word_counts):.1f}')\n",
    "ax1.set_xlabel('Number of Words')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Sentence Length Distribution (Words)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Character count distribution\n",
    "ax2.hist(train_char_counts, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "ax2.axvline(np.mean(train_char_counts), color='red', linestyle='--',\n",
    "            label=f'Mean: {np.mean(train_char_counts):.1f}')\n",
    "ax2.axvline(np.median(train_char_counts), color='green', linestyle='--',\n",
    "            label=f'Median: {np.median(train_char_counts):.1f}')\n",
    "ax2.set_xlabel('Number of Characters')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Sentence Length Distribution (Characters)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/length_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Length distribution plot saved to figures/length_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Character Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze character distribution\n",
    "all_chars = ''.join(train_texts)\n",
    "char_counter = Counter(all_chars)\n",
    "\n",
    "# Devanagari range: U+0900 to U+097F\n",
    "devanagari_chars = {char: count for char, count in char_counter.items()\n",
    "                    if '\\u0900' <= char <= '\\u097F'}\n",
    "\n",
    "# Top Devanagari characters\n",
    "top_devanagari = sorted(devanagari_chars.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "chars, counts = zip(*top_devanagari)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(range(len(chars)), counts, color='skyblue', edgecolor='black')\n",
    "plt.xticks(range(len(chars)), chars, fontsize=14)\n",
    "plt.xlabel('Character', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Top 30 Devanagari Characters', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/character_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "total_chars = len(all_chars)\n",
    "devanagari_count = sum(devanagari_chars.values())\n",
    "hindi_ratio = devanagari_count / total_chars\n",
    "\n",
    "print(f\"\\nðŸ“Š Character Statistics:\")\n",
    "print(f\"  Total characters: {total_chars:,}\")\n",
    "print(f\"  Devanagari characters: {devanagari_count:,}\")\n",
    "print(f\"  Hindi ratio: {hindi_ratio:.2%}\")\n",
    "print(f\"  Unique Devanagari characters: {len(devanagari_chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency\n",
    "all_words = [word for text in train_texts for word in text.split()]\n",
    "word_counter = Counter(all_words)\n",
    "\n",
    "# Top words\n",
    "top_words = word_counter.most_common(30)\n",
    "words, word_counts = zip(*top_words)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.barh(range(len(words)), word_counts, color='lightcoral', edgecolor='black')\n",
    "plt.yticks(range(len(words)), words, fontsize=11)\n",
    "plt.xlabel('Frequency', fontsize=12)\n",
    "plt.title('Top 30 Most Frequent Words', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/word_frequency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Word frequency plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Morphological Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze morphological markers\n",
    "case_markers = ['à¤¨à¥‡', 'à¤•à¥‹', 'à¤¸à¥‡', 'à¤®à¥‡à¤‚', 'à¤ªà¤°', 'à¤•à¤¾', 'à¤•à¥€', 'à¤•à¥‡']\n",
    "marker_counts = {marker: sum(text.count(marker) for text in train_texts) \n",
    "                 for marker in case_markers}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "markers = list(marker_counts.keys())\n",
    "counts = list(marker_counts.values())\n",
    "\n",
    "plt.bar(markers, counts, color='mediumseagreen', edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Case Marker', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Hindi Case Marker Distribution', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/case_markers.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Case Marker Statistics:\")\n",
    "for marker, count in sorted(marker_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {marker}: {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess data quality\n",
    "def assess_quality(texts):\n",
    "    \"\"\"Assess corpus quality\"\"\"\n",
    "    quality_stats = {\n",
    "        'too_short': 0,\n",
    "        'too_long': 0,\n",
    "        'low_hindi_ratio': 0,\n",
    "        'has_urls': 0,\n",
    "        'clean': 0\n",
    "    }\n",
    "    \n",
    "    for text in texts:\n",
    "        word_count = len(text.split())\n",
    "        \n",
    "        # Check length\n",
    "        if word_count < 5:\n",
    "            quality_stats['too_short'] += 1\n",
    "            continue\n",
    "        if word_count > 200:\n",
    "            quality_stats['too_long'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Check Hindi ratio\n",
    "        devanagari = sum(1 for c in text if '\\u0900' <= c <= '\\u097F')\n",
    "        hindi_ratio = devanagari / len(text) if text else 0\n",
    "        \n",
    "        if hindi_ratio < 0.7:\n",
    "            quality_stats['low_hindi_ratio'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Check for URLs\n",
    "        if 'http' in text or 'www' in text:\n",
    "            quality_stats['has_urls'] += 1\n",
    "            continue\n",
    "        \n",
    "        quality_stats['clean'] += 1\n",
    "    \n",
    "    return quality_stats\n",
    "\n",
    "quality_stats = assess_quality(train_texts)\n",
    "\n",
    "# Plot quality distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "labels = list(quality_stats.keys())\n",
    "values = list(quality_stats.values())\n",
    "colors = ['red', 'orange', 'yellow', 'pink', 'green']\n",
    "\n",
    "plt.pie(values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Data Quality Distribution', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/data_quality.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Data Quality Assessment:\")\n",
    "total = sum(quality_stats.values())\n",
    "for category, count in quality_stats.items():\n",
    "    pct = (count / total) * 100\n",
    "    print(f\"  {category}: {count:,} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary = {\n",
    "    'dataset_statistics': train_stats,\n",
    "    'quality_assessment': quality_stats,\n",
    "    'case_markers': marker_counts,\n",
    "    'top_words': dict(top_words[:50]),\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('../data/corpus_statistics.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Summary statistics saved to data/corpus_statistics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook analyzed:\n",
    "- âœ… Corpus size and statistics\n",
    "- âœ… Length distributions\n",
    "- âœ… Character and word frequencies\n",
    "- âœ… Morphological markers\n",
    "- âœ… Data quality\n",
    "\n",
    "All figures saved to `figures/` directory for thesis use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
