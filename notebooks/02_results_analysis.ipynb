{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis for Hindi BabyLM\n",
    "\n",
    "This notebook analyzes experimental results:\n",
    "- Training curves comparison\n",
    "- Evaluation metrics visualization  \n",
    "- Statistical significance testing\n",
    "- Model comparison\n",
    "- Thesis figure generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\n\n# Add project root to Python path - multiple fallback strategies\nnotebook_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n\n# Try to find project root by looking for key directories\ncurrent = notebook_dir\nwhile current != current.parent:\n    if (current / 'src').exists() and (current / 'data').exists():\n        project_root = current\n        break\n    current = current.parent\nelse:\n    # Fallback: assume we're in notebooks/ subdirectory\n    project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n\n# Add to path if not already there\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nprint(f\"Project root: {project_root}\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Project imports\nfrom src.analysis.results_analyzer import ResultsAnalyzer, analyze_experiments\nfrom src.analysis.visualization_utils import ThesisPlotter\n\n# Set plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"âœ… Imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results analyzer\n",
    "analyzer = analyze_experiments(results_dir='../results')\n",
    "\n",
    "print(f\"Loaded {len(analyzer.experiments)} experiments:\")\n",
    "for exp_name in analyzer.experiments.keys():\n",
    "    print(f\"  - {exp_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for all experiments\n",
    "fig = analyzer.plot_training_curves(\n",
    "    metrics=['loss', 'perplexity'],\n",
    "    save_path='../figures/training_curves.png'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "print(\"âœ… Training curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IndicGLUE Evaluation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare IndicGLUE performance\n",
    "fig = analyzer.plot_evaluation_comparison(\n",
    "    eval_type='indicglue',\n",
    "    save_path='../figures/indicglue_comparison.png'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "print(\"âœ… IndicGLUE comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MultiBLiMP Syntactic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MultiBLiMP performance\n",
    "fig = analyzer.plot_evaluation_comparison(\n",
    "    eval_type='multiblimp',\n",
    "    save_path='../figures/multiblimp_comparison.png'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "print(\"âœ… MultiBLiMP comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Morphological Probes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare morphological probe performance\n",
    "fig = analyzer.plot_evaluation_comparison(\n",
    "    eval_type='probes',\n",
    "    save_path='../figures/morphological_probes_comparison.png'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "print(\"âœ… Morphological probes comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two best models statistically\n",
    "experiments = list(analyzer.experiments.keys())\n",
    "\n",
    "if len(experiments) >= 2:\n",
    "    exp1, exp2 = experiments[0], experiments[1]\n",
    "    \n",
    "    print(f\"Comparing: {exp1} vs {exp2}\\n\")\n",
    "    \n",
    "    # Statistical comparison\n",
    "    comparison = analyzer.compare_models_statistically(\n",
    "        exp1, exp2,\n",
    "        metric='accuracy',\n",
    "        eval_type='indicglue'\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ“Š Statistical Comparison:\")\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"  {exp1} mean: {comparison['summary']['exp1_mean']:.4f} Â± {comparison['summary']['exp1_std']:.4f}\")\n",
    "    print(f\"  {exp2} mean: {comparison['summary']['exp2_mean']:.4f} Â± {comparison['summary']['exp2_std']:.4f}\")\n",
    "    print(f\"  Difference: {comparison['summary']['difference']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPaired t-test:\")\n",
    "    print(f\"  p-value: {comparison['t_test']['p_value']:.4f}\")\n",
    "    print(f\"  Significant: {'Yes âœ“' if comparison['t_test']['significant'] else 'No âœ—'}\")\n",
    "    \n",
    "    print(f\"\\nWilcoxon test:\")\n",
    "    print(f\"  p-value: {comparison['wilcoxon']['p_value']:.4f}\")\n",
    "    print(f\"  Significant: {'Yes âœ“' if comparison['wilcoxon']['significant'] else 'No âœ—'}\")\n",
    "    \n",
    "    print(f\"\\nEffect Size:\")\n",
    "    print(f\"  Cohen's d: {comparison['effect_size']['cohens_d']:.4f}\")\n",
    "    print(f\"  Interpretation: {comparison['effect_size']['interpretation']}\")\n",
    "    \n",
    "    print(f\"\\nBootstrap 95% CI:\")\n",
    "    print(f\"  [{comparison['bootstrap_ci']['lower']:.4f}, {comparison['bootstrap_ci']['upper']:.4f}]\")\n",
    "else:\n",
    "    print(\"Need at least 2 experiments for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate LaTeX Tables for Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table for IndicGLUE\n",
    "latex_table = analyzer.generate_latex_table(\n",
    "    eval_type='indicglue',\n",
    "    metric='accuracy',\n",
    "    caption='IndicGLUE Benchmark Results',\n",
    "    label='tab:indicglue_results',\n",
    "    save_path='../tables/indicglue_results.tex'\n",
    ")\n",
    "\n",
    "print(\"IndicGLUE LaTeX Table:\")\n",
    "print(latex_table)\n",
    "print(\"\\nâœ… LaTeX table saved to tables/indicglue_results.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table for MultiBLiMP\n",
    "latex_table = analyzer.generate_latex_table(\n",
    "    eval_type='multiblimp',\n",
    "    metric='accuracy',\n",
    "    caption='MultiBLiMP Syntactic Phenomena Results',\n",
    "    label='tab:multiblimp_results',\n",
    "    save_path='../tables/multiblimp_results.tex'\n",
    ")\n",
    "\n",
    "print(\"âœ… MultiBLiMP LaTeX table saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Layer-wise Probe Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer-wise probe results\n",
    "plotter = ThesisPlotter(style='thesis')\n",
    "\n",
    "# Example: Case detection probe\n",
    "# (Replace with actual data from your experiments)\n",
    "layer_results = {i: 0.5 + 0.03 * i + np.random.normal(0, 0.02) \n",
    "                for i in range(13)}\n",
    "\n",
    "fig = plotter.plot_layer_wise_probe_results(\n",
    "    layer_results,\n",
    "    probe_name='Case Detection',\n",
    "    title='Layer-wise Case Detection Accuracy',\n",
    "    save_path='../figures/layer_wise_case_probe.png'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "print(\"âœ… Layer-wise probe visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Size vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance vs model size\n",
    "# (Replace with actual data)\n",
    "model_sizes = [50, 110, 350]  # Millions of parameters\n",
    "accuracies = [0.72, 0.78, 0.82]\n",
    "model_names = ['Tiny', 'Small', 'Medium']\n",
    "\n",
    "fig = plotter.plot_performance_vs_model_size(\n",
    "    model_sizes,\n",
    "    accuracies,\n",
    "    model_names,\n",
    "    title='Model Size vs IndicGLUE Performance',\n",
    "    save_path='../figures/model_size_vs_performance.png'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "print(\"âœ… Model size comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Individual Experiment Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reports for each experiment\n",
    "for exp_name in analyzer.experiments.keys():\n",
    "    report = analyzer.generate_summary_report(\n",
    "        exp_name,\n",
    "        save_path=f'../reports/{exp_name}_report.md'\n",
    "    )\n",
    "    print(f\"âœ… Generated report for {exp_name}\")\n",
    "\n",
    "print(\"\\nAll reports saved to reports/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook analyzed:\n",
    "- âœ… Training curves and convergence\n",
    "- âœ… Evaluation metrics across benchmarks\n",
    "- âœ… Statistical significance of differences\n",
    "- âœ… LaTeX tables for thesis\n",
    "- âœ… Layer-wise probing analysis\n",
    "- âœ… Model size comparisons\n",
    "\n",
    "All figures and tables ready for thesis inclusion!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}