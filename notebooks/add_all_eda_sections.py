#!/usr/bin/env python3
"""
Comprehensive script to add all remaining EDA sections to the notebook.
This creates a publication-quality comprehensive data exploration notebook.
"""

import json
from pathlib import Path

# Load existing notebook
notebook_path = Path(__file__).parent / '01_data_exploration.ipynb'
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook = json.load(f)

# Store all new cells
all_new_cells = []

def add_markdown(text):
    """Add markdown cell."""
    all_new_cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [text]
    })

def add_code(code):
    """Add code cell."""
    all_new_cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [code]
    })

# ============================================================================
# SECTION 3: ADVANCED DISTRIBUTION ANALYSIS
# ============================================================================

add_markdown([
    "---\n",
    "## Section 3: Advanced Distribution Analysis\n",
    "\n",
    "In-depth analysis of document length distributions, vocabulary growth curves (Heap's law), and statistical tests."
])

add_code([
    "# ============================================================================\n",
    "# DOCUMENT LENGTH DISTRIBUTIONS WITH KDE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìä Analyzing document length distributions...\\n\")\n",
    "\n",
    "# Extract word counts per source\n",
    "length_data = {}\n",
    "for source_name, texts in sources.items():\n",
    "    word_counts = [len(text.split()) for text in texts]\n",
    "    length_data[source_name] = word_counts\n",
    "\n",
    "# Create multi-panel visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Panel 1: Overlaid histograms\n",
    "ax = axes[0, 0]\n",
    "colors = sns.color_palette('Set2', len(sources))\n",
    "for (source, lengths), color in zip(length_data.items(), colors):\n",
    "    ax.hist(lengths, bins=30, alpha=0.5, label=source, color=color, edgecolor='black')\n",
    "ax.set_xlabel('Document Length (words)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Document Length Distribution (Histogram)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: KDE plots\n",
    "ax = axes[0, 1]\n",
    "for (source, lengths), color in zip(length_data.items(), colors):\n",
    "    if len(lengths) > 1:\n",
    "        pd.Series(lengths).plot.kde(ax=ax, label=source, color=color, linewidth=2)\n",
    "ax.set_xlabel('Document Length (words)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Document Length Distribution (KDE)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Box plots\n",
    "ax = axes[1, 0]\n",
    "bp = ax.boxplot(length_data.values(), labels=length_data.keys(), patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.set_ylabel('Document Length (words)')\n",
    "ax.set_title('Document Length Distribution (Boxplot)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(ax.get_xticklabels(), rotation=15)\n",
    "\n",
    "# Panel 4: Log-scale histogram\n",
    "ax = axes[1, 1]\n",
    "for (source, lengths), color in zip(length_data.items(), colors):\n",
    "    # Filter out zero or negative values for log scale\n",
    "    lengths_positive = [l for l in lengths if l > 0]\n",
    "    ax.hist(lengths_positive, bins=30, alpha=0.5, label=source, color=color, edgecolor='black')\n",
    "ax.set_xlabel('Document Length (words)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Document Length Distribution (Log Scale)', fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'length_distributions_comprehensive.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comprehensive length distribution plots saved\")"
])

add_code([
    "# ============================================================================\n",
    "# STATISTICAL TESTS ON LENGTH DISTRIBUTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Performing statistical tests on distributions...\\n\")\n",
    "\n",
    "# Test for normality (Shapiro-Wilk test)\n",
    "print(\"NORMALITY TESTS (Shapiro-Wilk):\")\n",
    "print(\"=\"*60)\n",
    "for source, lengths in length_data.items():\n",
    "    if len(lengths) > 3:  # Need at least 3 samples\n",
    "        # Sample up to 5000 points for the test (Shapiro-Wilk limitation)\n",
    "        sample = lengths[:min(5000, len(lengths))]\n",
    "        stat, p_value = stats.shapiro(sample)\n",
    "        is_normal = \"YES\" if p_value > 0.05 else \"NO\"\n",
    "        print(f\"  {source:12s}: W={stat:.4f}, p={p_value:.4f} -> Normal? {is_normal}\")\n",
    "\n",
    "# Calculate skewness and kurtosis\n",
    "print(\"\\nDISTRIBUTION SHAPE METRICS:\")\n",
    "print(\"=\"*60)\n",
    "for source, lengths in length_data.items():\n",
    "    skew = stats.skew(lengths)\n",
    "    kurt = stats.kurtosis(lengths)\n",
    "    print(f\"  {source:12s}: Skewness={skew:>6.2f}, Kurtosis={kurt:>6.2f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Skewness > 0: Right-skewed (long tail on right)\")\n",
    "print(\"  - Skewness < 0: Left-skewed (long tail on left)\")\n",
    "print(\"  - Kurtosis > 0: Heavy-tailed (more outliers)\")\n",
    "print(\"  - Kurtosis < 0: Light-tailed (fewer outliers)\")\n",
    "\n",
    "# Pairwise Kol mogorov-Smirnov test\n",
    "print(\"\\n\\nPAIRWISE DISTRIBUTION SIMILARITY (Kolmogorov-Smirnov Test):\")\n",
    "print(\"=\"*60)\n",
    "source_list = list(length_data.keys())\n",
    "for i in range(len(source_list)):\n",
    "    for j in range(i+1, len(source_list)):\n",
    "        s1, s2 = source_list[i], source_list[j]\n",
    "        stat, p_value = stats.ks_2samp(length_data[s1], length_data[s2])\n",
    "        similar = \"YES\" if p_value > 0.05 else \"NO\"\n",
    "        print(f\"  {s1} vs {s2}: D={stat:.4f}, p={p_value:.4f} -> Similar? {similar}\")"
])

add_code([
    "# ============================================================================\n",
    "# VOCABULARY GROWTH CURVES (HEAP'S LAW)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìà Analyzing vocabulary growth (Heap's Law: V = K * N^Œ≤)...\\n\")\n",
    "\n",
    "def compute_vocab_growth(texts, sample_points=50):\n",
    "    \"\"\"\n",
    "    Compute vocabulary growth curve.\n",
    "    Returns: (token_counts, vocab_sizes)\n",
    "    \"\"\"\n",
    "    all_words = [word for text in texts for word in text.split()]\n",
    "    \n",
    "    # Sample at regular intervals\n",
    "    total = len(all_words)\n",
    "    if total == 0:\n",
    "        return [], []\n",
    "    \n",
    "    sample_indices = np.linspace(100, total, min(sample_points, total // 10), dtype=int)\n",
    "    \n",
    "    vocab_sizes = []\n",
    "    token_counts = []\n",
    "    \n",
    "    seen_words = set()\n",
    "    for idx in sample_indices:\n",
    "        seen_words.update(all_words[:idx])\n",
    "        vocab_sizes.append(len(seen_words))\n",
    "        token_counts.append(idx)\n",
    "    \n",
    "    return token_counts, vocab_sizes\n",
    "\n",
    "# Compute growth curves for each source\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "colors = sns.color_palette('Set2', len(sources))\n",
    "\n",
    "# Linear scale\n",
    "for (source, texts), color in zip(sources.items(), colors):\n",
    "    tokens, vocab = compute_vocab_growth(texts)\n",
    "    if tokens:\n",
    "        ax1.plot(tokens, vocab, marker='o', markersize=3, label=source, color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Number of Tokens')\n",
    "ax1.set_ylabel('Vocabulary Size')\n",
    "ax1.set_title(\"Vocabulary Growth Curve (Heap's Law)\", fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Log-log scale (Heap's law should be linear in log-log)\n",
    "for (source, texts), color in zip(sources.items(), colors):\n",
    "    tokens, vocab = compute_vocab_growth(texts)\n",
    "    if tokens:\n",
    "        # Fit power law: V = K * N^beta\n",
    "        log_tokens = np.log(tokens)\n",
    "        log_vocab = np.log(vocab)\n",
    "        \n",
    "        # Linear regression in log space\n",
    "        if len(log_tokens) > 1:\n",
    "            slope, intercept = np.polyfit(log_tokens, log_vocab, 1)\n",
    "            beta = slope\n",
    "            K = np.exp(intercept)\n",
    "            \n",
    "            ax2.scatter(tokens, vocab, alpha=0.6, color=color, s=30)\n",
    "            ax2.plot(tokens, vocab, label=f\"{source} (Œ≤={beta:.2f})\", color=color, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Number of Tokens (log scale)')\n",
    "ax2.set_ylabel('Vocabulary Size (log scale)')\n",
    "ax2.set_title(\"Heap's Law: Log-Log Scale\", fontweight='bold')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'vocabulary_growth.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Vocabulary growth curves saved\")\n",
    "print(\"\\nNote: Heap's Law states V = K * N^Œ≤\")\n",
    "print(\"  - Œ≤ typically ranges from 0.4 to 0.6 for natural language\")\n",
    "print(\"  - Higher Œ≤ indicates more vocabulary diversity\")"
])

# ============================================================================
# SECTION 4: DEEP CHARACTER & SCRIPT ANALYSIS
# ============================================================================

add_markdown([
    "---\n",
    "## Section 4: Deep Character & Script Analysis\n",
    "\n",
    "Comprehensive analysis of Unicode blocks, Devanagari characters, matras (vowel diacritics), conjuncts, and script mixing."
])

add_code([
    "# ============================================================================\n",
    "# UNICODE BLOCK ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üî§ Analyzing Unicode blocks across sources...\\n\")\n",
    "\n",
    "def analyze_unicode_blocks(texts):\n",
    "    \"\"\"\n",
    "    Analyze Unicode block distribution in texts.\n",
    "    \"\"\"\n",
    "    all_text = ''.join(texts)\n",
    "    block_counts = Counter()\n",
    "    \n",
    "    for char in all_text:\n",
    "        block = get_unicode_block(char)\n",
    "        block_counts[block] += 1\n",
    "    \n",
    "    return block_counts\n",
    "\n",
    "# Analyze each source\n",
    "unicode_stats = {}\n",
    "for source, texts in sources.items():\n",
    "    unicode_stats[source] = analyze_unicode_blocks(texts)\n",
    "\n",
    "# Create stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Get all unique blocks\n",
    "all_blocks = set()\n",
    "for stats in unicode_stats.values():\n",
    "    all_blocks.update(stats.keys())\n",
    "all_blocks = sorted(all_blocks)\n",
    "\n",
    "# Prepare data for stacked bar\n",
    "source_names = list(unicode_stats.keys())\n",
    "block_data = {block: [] for block in all_blocks}\n",
    "\n",
    "for source in source_names:\n",
    "    total = sum(unicode_stats[source].values())\n",
    "    for block in all_blocks:\n",
    "        pct = (unicode_stats[source].get(block, 0) / total * 100) if total > 0 else 0\n",
    "        block_data[block].append(pct)\n",
    "\n",
    "# Plot stacked bars\n",
    "x = np.arange(len(source_names))\n",
    "width = 0.6\n",
    "bottom = np.zeros(len(source_names))\n",
    "\n",
    "colors_blocks = plt.cm.Set3(np.linspace(0, 1, len(all_blocks)))\n",
    "\n",
    "for block, color in zip(all_blocks, colors_blocks):\n",
    "    ax.bar(x, block_data[block], width, label=block, bottom=bottom, color=color)\n",
    "    bottom += block_data[block]\n",
    "\n",
    "ax.set_ylabel('Percentage (%)')\n",
    "ax.set_title('Unicode Block Distribution by Source', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(source_names)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'unicode_blocks.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Unicode Block Statistics:\")\n",
    "print(\"=\"*60)\n",
    "for source in source_names:\n",
    "    print(f\"\\n{source}:\")\n",
    "    total = sum(unicode_stats[source].values())\n",
    "    for block in sorted(unicode_stats[source].keys(), key=lambda b: unicode_stats[source][b], reverse=True):\n",
    "        count = unicode_stats[source][block]\n",
    "        pct = count / total * 100\n",
    "        print(f\"  {block:15s}: {count:>8,} ({pct:>5.1f}%)\")"
])

add_code([
    "# ============================================================================\n",
    "# DEVANAGARI CHARACTER DISTRIBUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüî† Analyzing Devanagari character frequencies...\\n\")\n",
    "\n",
    "# Collect all Devanagari characters from all sources\n",
    "all_devanagari = Counter()\n",
    "for texts in sources.values():\n",
    "    all_text = ''.join(texts)\n",
    "    for char in all_text:\n",
    "        if is_devanagari(char):\n",
    "            all_devanagari[char] += 1\n",
    "\n",
    "# Top 40 Devanagari characters\n",
    "top_chars = all_devanagari.most_common(40)\n",
    "chars, counts = zip(*top_chars) if top_chars else ([], [])\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "ax1.bar(range(len(chars)), counts, color='skyblue', edgecolor='black')\n",
    "ax1.set_xticks(range(len(chars)))\n",
    "ax1.set_xticklabels(chars, fontsize=14)\n",
    "ax1.set_xlabel('Character')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Top 40 Devanagari Characters', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative distribution\n",
    "cumsum = np.cumsum(counts)\n",
    "cumsum_pct = cumsum / cumsum[-1] * 100 if len(cumsum) > 0 else []\n",
    "\n",
    "ax2.plot(range(len(chars)), cumsum_pct, marker='o', linewidth=2, markersize=4, color='coral')\n",
    "ax2.axhline(y=80, color='red', linestyle='--', label='80% coverage')\n",
    "ax2.axhline(y=90, color='orange', linestyle='--', label='90% coverage')\n",
    "ax2.set_xlabel('Character Rank')\n",
    "ax2.set_ylabel('Cumulative Coverage (%)')\n",
    "ax2.set_title('Cumulative Character Coverage', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'devanagari_characters.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find characters for 80% and 90% coverage\n",
    "if len(cumsum_pct) > 0:\n",
    "    idx_80 = next((i for i, pct in enumerate(cumsum_pct) if pct >= 80), len(cumsum_pct)-1)\n",
    "    idx_90 = next((i for i, pct in enumerate(cumsum_pct) if pct >= 90), len(cumsum_pct)-1)\n",
    "    print(f\"Characters needed for 80% coverage: {idx_80 + 1}\")\n",
    "    print(f\"Characters needed for 90% coverage: {idx_90 + 1}\")\n",
    "    print(f\"Total unique Devanagari characters: {len(all_devanagari)}\")"
])

add_code([
    "# ============================================================================\n",
    "# MATRAS (VOWEL DIACRITICS) ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n‚ú® Analyzing matras (vowel diacritics)...\\n\")\n",
    "\n",
    "# Devanagari matras (dependent vowel signs)\n",
    "matras = {\n",
    "    '‡§æ': 'aa (long a)',\n",
    "    '‡§ø': 'i (short i)',\n",
    "    '‡•Ä': 'ii (long i)',\n",
    "    '‡•Å': 'u (short u)',\n",
    "    '‡•Ç': 'uu (long u)',\n",
    "    '‡•á': 'e',\n",
    "    '‡•à': 'ai',\n",
    "    '‡•ã': 'o',\n",
    "    '‡•å': 'au',\n",
    "    '‡§Ç': 'anusvara',\n",
    "    '‡§É': 'visarga',\n",
    "    '‡§Å': 'chandrabindu',\n",
    "    '‡•ç': 'halant (virama)',\n",
    "}\n",
    "\n",
    "# Count matras across sources\n",
    "matra_counts = {source: Counter() for source in sources.keys()}\n",
    "\n",
    "for source, texts in sources.items():\n",
    "    all_text = ''.join(texts)\n",
    "    for matra in matras.keys():\n",
    "        matra_counts[source][matra] = all_text.count(matra)\n",
    "\n",
    "# Create heatmap\n",
    "source_names = list(sources.keys())\n",
    "matra_list = list(matras.keys())\n",
    "\n",
    "# Build matrix\n",
    "matrix = np.zeros((len(matra_list), len(source_names)))\n",
    "for i, matra in enumerate(matra_list):\n",
    "    for j, source in enumerate(source_names):\n",
    "        matrix[i, j] = matra_counts[source][matra]\n",
    "\n",
    "# Normalize by column (per source) for percentage\n",
    "matrix_pct = matrix / matrix.sum(axis=0, keepdims=True) * 100\n",
    "matrix_pct = np.nan_to_num(matrix_pct)  # Replace NaN with 0\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(matrix_pct, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(source_names)))\n",
    "ax.set_yticks(np.arange(len(matra_list)))\n",
    "ax.set_xticklabels(source_names)\n",
    "# Create labels with both matra and description\n",
    "matra_labels = [f\"{m} ({matras[m]})\" for m in matra_list]\n",
    "ax.set_yticklabels(matra_labels)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(len(matra_list)):\n",
    "    for j in range(len(source_names)):\n",
    "        count = int(matrix[i, j])\n",
    "        pct = matrix_pct[i, j]\n",
    "        if count > 0:\n",
    "            text = ax.text(j, i, f'{count}\\n({pct:.1f}%)',\n",
    "                          ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "ax.set_title('Matras (Vowel Diacritics) Heatmap', fontsize=14, fontweight='bold')\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label('Percentage (%)', rotation=270, labelpad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'matras_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Matras heatmap saved\")"
])

add_code([
    "# ============================================================================\n",
    "# SCRIPT MIXING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüåê Analyzing script mixing (Hindi vs English)...\\n\")\n",
    "\n",
    "def analyze_script_mixing(texts):\n",
    "    \"\"\"\n",
    "    Analyze the extent of script mixing in texts.\n",
    "    Returns: (hindi_ratio, has_mixing_pct, avg_english_words_per_sent)\n",
    "    \"\"\"\n",
    "    hindi_ratios = []\n",
    "    mixed_count = 0\n",
    "    english_word_counts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        hindi_ratio = calculate_hindi_ratio(text)\n",
    "        hindi_ratios.append(hindi_ratio)\n",
    "        \n",
    "        # Check if text has mixing (both Hindi and Latin)\n",
    "        has_hindi = any(is_devanagari(c) for c in text)\n",
    "        has_latin = any(c.isalpha() and c.isascii() for c in text)\n",
    "        if has_hindi and has_latin:\n",
    "            mixed_count += 1\n",
    "        \n",
    "        # Count English words (rough heuristic)\n",
    "        words = text.split()\n",
    "        english_words = [w for w in words if any(c.isascii() and c.isalpha() for c in w)]\n",
    "        english_word_counts.append(len(english_words))\n",
    "    \n",
    "    return {\n",
    "        'avg_hindi_ratio': np.mean(hindi_ratios) if hindi_ratios else 0,\n",
    "        'min_hindi_ratio': np.min(hindi_ratios) if hindi_ratios else 0,\n",
    "        'max_hindi_ratio': np.max(hindi_ratios) if hindi_ratios else 0,\n",
    "        'mixed_pct': (mixed_count / len(texts) * 100) if texts else 0,\n",
    "        'avg_english_words': np.mean(english_word_counts) if english_word_counts else 0,\n",
    "    }\n",
    "\n",
    "# Analyze script mixing for each source\n",
    "mixing_stats = {}\n",
    "for source, texts in sources.items():\n",
    "    mixing_stats[source] = analyze_script_mixing(texts)\n",
    "\n",
    "df_mixing = pd.DataFrame(mixing_stats).T\n",
    "\n",
    "print(\"Script Mixing Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(df_mixing.to_string())\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - avg_hindi_ratio: Average proportion of Devanagari characters\")\n",
    "print(\"  - mixed_pct: Percentage of documents with both Hindi and English\")\n",
    "print(\"  - avg_english_words: Average number of English words per document\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Hindi ratio by source\n",
    "sources_list = list(mixing_stats.keys())\n",
    "hindi_ratios = [mixing_stats[s]['avg_hindi_ratio'] * 100 for s in sources_list]\n",
    "\n",
    "bars = ax1.bar(sources_list, hindi_ratios, color=sns.color_palette('Set2', len(sources_list)), edgecolor='black')\n",
    "ax1.axhline(y=90, color='red', linestyle='--', label='90% threshold')\n",
    "ax1.set_ylabel('Hindi Character Ratio (%)')\n",
    "ax1.set_title('Average Hindi Content by Source', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Mixed content percentage\n",
    "mixed_pcts = [mixing_stats[s]['mixed_pct'] for s in sources_list]\n",
    "\n",
    "bars = ax2.bar(sources_list, mixed_pcts, color=sns.color_palette('Set2', len(sources_list)), edgecolor='black')\n",
    "ax2.set_ylabel('Percentage (%)')\n",
    "ax2.set_title('Documents with Script Mixing', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'script_mixing.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
])

# Save checkpoint
print(\"Building comprehensive EDA notebook... (Sections 3-4 complete)\")\n\n# Continue with remaining sections\n# ============================================================================\n# SECTION 5: ADVANCED WORD-LEVEL ANALYSIS\n# ============================================================================\n\nadd_markdown([\n    \"---\\n\",\n    \"## Section 5: Advanced Word-Level Analysis\\n\",\n    \"\\n\",\n    \"Zipf's law, hapax legomena, OOV rates, word length distributions, and frequency band analysis.\"\n])\n\nadd_code([\n    \"# ============================================================================\\n\",\n    \"# ZIPF'S LAW VALIDATION\\n\",\n    \"# ============================================================================\\n\",\n    \"\\n\",\n    \"print(\\\"üìä Validating Zipf's Law...\\\\n\\\")\\n\",\n    \"\\n\",\n    \"# Collect all words and compute frequencies\\n\",\n    \"all_words = [word for texts in sources.values() for text in texts for word in text.split()]\\n\",\n    \"word_freq = Counter(all_words)\\n\",\n    \"\\n\",\n    \"# Sort by frequency\\n\",\n    \"sorted_words = word_freq.most_common()\\n\",\n    \"\\n\",\n    \"if sorted_words:\\n\",\n    \"    ranks = np.arange(1, len(sorted_words) + 1)\\n\",\n    \"    frequencies = np.array([freq for word, freq in sorted_words])\\n\",\n    \"    \\n\",\n    \"    # Create Zipf plot\\n\",\n    \"    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n    \"    \\n\",\n    \"    # Regular scale\\n\",\n    \"    ax1.scatter(ranks[:1000], frequencies[:1000], alpha=0.5, s=20, color='steelblue')\\n\",\n    \"    ax1.set_xlabel('Rank')\\n\",\n    \"    ax1.set_ylabel('Frequency')\\n\",\n    \"    ax1.set_title('Zipf\\\\'s Law: Rank vs Frequency (Top 1000)', fontweight='bold')\\n\",\n    \"    ax1.grid(True, alpha=0.3)\\n\",\n    \"    \\n\",\n    \"    # Log-log scale (should be linear for Zipf's law)\\n\",\n    \"    ax2.scatter(ranks, frequencies, alpha=0.5, s=10, color='coral')\\n\",\n    \"    \\n\",\n    \"    # Fit power law in log space\\n\",\n    \"    log_ranks = np.log(ranks)\\n\",\n    \"    log_freqs = np.log(frequencies)\\n\",\n    \"    slope, intercept = np.polyfit(log_ranks, log_freqs, 1)\\n\",\n    \"    \\n\",\n    \"    # Plot fitted line\\n\",\n    \"    fitted_freqs = np.exp(intercept + slope * log_ranks)\\n\",\n    \"    ax2.plot(ranks, fitted_freqs, 'r--', linewidth=2, label=f'Fit: slope={slope:.2f}')\\n\",\n    \"    \\n\",\n    \"    ax2.set_xlabel('Rank (log scale)')\\n\",\n    \"    ax2.set_ylabel('Frequency (log scale)')\\n\",\n    \"    ax2.set_title('Zipf\\\\'s Law: Log-Log Plot', fontweight='bold')\\n\",\n    \"    ax2.set_xscale('log')\\n\",\n    \"    ax2.set_yscale('log')\\n\",\n    \"    ax2.legend()\\n\",\n    \"    ax2.grid(True, alpha=0.3, which='both')\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.savefig(FIGURES_DIR / 'zipf_law.png', dpi=300, bbox_inches='tight')\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"‚úÖ Zipf's law plot saved\\\")\\n\",\n    \"    print(f\\\"\\\\nZipf exponent: {-slope:.3f}\\\")\\n\",\n    \"    print(\\\"Note: For natural language, exponent is typically close to 1.0\\\")\\n\",\n    \"    \\n\",\n    \"    # Show top 20 words\\n\",\n    \"    print(\\\"\\\\nTop 20 most frequent words:\\\")\\n\",\n    \"    for i, (word, freq) in enumerate(sorted_words[:20], 1):\\n\",\n    \"        print(f\\\"  {i:2d}. {word:15s} {freq:>6,} occurrences\\\")\"\n])\n\nadd_code([\n    \"# ============================================================================\\n\",\n    \"# HAPAX LEGOMENA & DIS LEGOMENA ANALYSIS\\n\",\n    \"# ============================================================================\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüìñ Analyzing hapax and dis legomena...\\\\n\\\")\\n\",\n    \"\\n\",\n    \"# Per-source analysis\\n\",\n    \"hapax_stats = {}\\n\",\n    \"for source, texts in sources.items():\\n\",\n    \"    words = [word for text in texts for word in text.split()]\\n\",\n    \"    word_counts = Counter(words)\\n\",\n    \"    \\n\",\n    \"    hapax = [w for w, c in word_counts.items() if c == 1]\\n\",\n    \"    dis = [w for w, c in word_counts.items() if c == 2]\\n\",\n    \"    \\n\",\n    \"    total_words = len(words)\\n\",\n    \"    total_types = len(word_counts)\\n\",\n    \"    \\n\",\n    \"    hapax_stats[source] = {\\n\",\n    \"        'hapax_count': len(hapax),\\n\",\n    \"        'hapax_pct': len(hapax) / total_types * 100 if total_types > 0 else 0,\\n\",\n    \"        'dis_count': len(dis),\\n\",\n    \"        'dis_pct': len(dis) / total_types * 100 if total_types > 0 else 0,\\n\",\n    \"        'total_types': total_types,\\n\",\n    \"    }\\n\",\n    \"\\n\",\n    \"df_hapax = pd.DataFrame(hapax_stats).T\\n\",\n    \"\\n\",\n    \"print(\\\"Hapax & Dis Legomena Statistics:\\\")\\n\",\n    \"print(\\\"=\\\"*80)\\n\",\n    \"print(df_hapax.to_string())\\n\",\n    \"print(\\\"\\\\nInterpretation:\\\")\\n\",\n    \"print(\\\"  - Hapax legomena: Words appearing exactly once (indicates vocabulary richness)\\\")\\n\",\n    \"print(\\\"  - Dis legomena: Words appearing exactly twice\\\")\\n\",\n    \"print(\\\"  - Higher percentages suggest more diverse/technical vocabulary\\\")\\n\",\n    \"\\n\",\n    \"# Visualize\\n\",\n    \"fig, ax = plt.subplots(figsize=(10, 6))\\n\",\n    \"\\n\",\n    \"x = np.arange(len(sources))\\n\",\n    \"width = 0.35\\n\",\n    \"\\n\",\n    \"source_names = list(sources.keys())\\n\",\n    \"hapax_pcts = [hapax_stats[s]['hapax_pct'] for s in source_names]\\n\",\n    \"dis_pcts = [hapax_stats[s]['dis_pct'] for s in source_names]\\n\",\n    \"\\n\",\n    \"bars1 = ax.bar(x - width/2, hapax_pcts, width, label='Hapax Legomena', color='steelblue')\\n\",\n    \"bars2 = ax.bar(x + width/2, dis_pcts, width, label='Dis Legomena', color='coral')\\n\",\n    \"\\n\",\n    \"ax.set_ylabel('Percentage of Vocabulary (%)')\\n\",\n    \"ax.set_title('Hapax & Dis Legomena by Source', fontweight='bold')\\n\",\n    \"ax.set_xticks(x)\\n\",\n    \"ax.set_xticklabels(source_names)\\n\",\n    \"ax.legend()\\n\",\n    \"ax.grid(True, alpha=0.3, axis='y')\\n\",\n    \"\\n\",\n    \"# Add value labels\\n\",\n    \"for bars in [bars1, bars2]:\\n\",\n    \"    for bar in bars:\\n\",\n    \"        height = bar.get_height()\\n\",\n    \"        ax.text(bar.get_x() + bar.get_width()/2., height,\\n\",\n    \"                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.savefig(FIGURES_DIR / 'hapax_dis_legomena.png', dpi=300, bbox_inches='tight')\\n\",\n    \"plt.show()\"\n])\n\nadd_code([\n    \"# ============================================================================\\n\",\n    \"# WORD LENGTH DISTRIBUTION\\n\",\n    \"# ============================================================================\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüìè Analyzing word length distributions...\\\\n\\\")\\n\",\n    \"\\n\",\n    \"# Calculate word lengths per source\\n\",\n    \"word_lengths = {}\\n\",\n    \"for source, texts in sources.items():\\n\",\n    \"    words = [word for text in texts for word in text.split()]\\n\",\n    \"    lengths = [len(word) for word in words]\\n\",\n    \"    word_lengths[source] = lengths\\n\",\n    \"\\n\",\n    \"# Create violin plot\\n\",\n    \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n    \"\\n\",\n    \"# Violin plot\\n\",\n    \"positions = list(range(len(sources)))\\n\",\n    \"parts = ax1.violinplot(word_lengths.values(), positions=positions, \\n\",\n    \"                       showmeans=True, showmedians=True)\\n\",\n    \"ax1.set_xticks(positions)\\n\",\n    \"ax1.set_xticklabels(word_lengths.keys())\\n\",\n    \"ax1.set_ylabel('Word Length (characters)')\\n\",\n    \"ax1.set_title('Word Length Distribution (Violin Plot)', fontweight='bold')\\n\",\n    \"ax1.grid(True, alpha=0.3, axis='y')\\n\",\n    \"\\n\",\n    \"colors = sns.color_palette('Set2', len(sources))\\n\",\n    \"for pc, color in zip(parts['bodies'], colors):\\n\",\n    \"    pc.set_facecolor(color)\\n\",\n    \"    pc.set_alpha(0.7)\\n\",\n    \"\\n\",\n    \"# Histogram comparison\\n\",\n    \"for (source, lengths), color in zip(word_lengths.items(), colors):\\n\",\n    \"    ax2.hist(lengths, bins=range(0, 25), alpha=0.5, label=source, \\n\",\n    \"             color=color, edgecolor='black', density=True)\\n\",\n    \"\\n\",\n    \"ax2.set_xlabel('Word Length (characters)')\\n\",\n    \"ax2.set_ylabel('Density')\\n\",\n    \"ax2.set_title('Word Length Distribution (Histogram)', fontweight='bold')\\n\",\n    \"ax2.legend()\\n\",\n    \"ax2.grid(True, alpha=0.3)\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.savefig(FIGURES_DIR / 'word_length_distribution.png', dpi=300, bbox_inches='tight')\\n\",\n    \"plt.show()\\n\",\n    \"\\n\",\n    \"# Statistics\\n\",\n    \"print(\\\"Word Length Statistics:\\\")\\n\",\n    \"print(\\\"=\\\"*60)\\n\",\n    \"for source, lengths in word_lengths.items():\\n\",\n    \"    print(f\\\"{source}:\\\")\\n\",\n    \"    print(f\\\"  Mean: {np.mean(lengths):.2f} characters\\\")\\n\",\n    \"    print(f\\\"  Median: {np.median(lengths):.0f} characters\\\")\\n\",\n    \"    print(f\\\"  Std Dev: {np.std(lengths):.2f}\\\")\\n\",\n    \"    print()\"\n])\n\nadd_code([\n    \"# ============================================================================\\n\",\n    \"# OOV (OUT-OF-VOCABULARY) RATE ANALYSIS\\n\",\n    \"# ============================================================================\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüîç Calculating OOV rates...\\\\n\\\")\\n\",\n    \"\\n\",\n    \"# Get vocabularies\\n\",\n    \"train_words = [word for text in train_texts for word in text.split()]\\n\",\n    \"val_words = [word for text in val_texts for word in text.split()]\\n\",\n    \"test_words = [word for text in test_texts for word in text.split()]\\n\",\n    \"\\n\",\n    \"train_vocab = set(train_words)\\n\",\n    \"val_vocab = set(val_words)\\n\",\n    \"test_vocab = set(test_words)\\n\",\n    \"\\n\",\n    \"# Calculate OOV rates\\n\",\n    \"val_oov = test_vocab - train_vocab\\n\",\n    \"test_oov = test_vocab - train_vocab\\n\",\n    \"\\n\",\n    \"val_oov_rate = len(val_oov) / len(val_vocab) * 100 if val_vocab else 0\\n\",\n    \"test_oov_rate = len(test_oov) / len(test_vocab) * 100 if test_vocab else 0\\n\",\n    \"\\n\",\n    \"# Token-level OOV\\n\",\n    \"val_oov_tokens = sum(1 for w in val_words if w not in train_vocab)\\n\",\n    \"test_oov_tokens = sum(1 for w in test_words if w not in train_vocab)\\n\",\n    \"\\n\",\n    \"val_oov_token_rate = val_oov_tokens / len(val_words) * 100 if val_words else 0\\n\",\n    \"test_oov_token_rate = test_oov_tokens / len(test_words) * 100 if test_words else 0\\n\",\n    \"\\n\",\n    \"print(\\\"OOV Analysis:\\\")\\n\",\n    \"print(\\\"=\\\"*60)\\n\",\n    \"print(f\\\"Training vocabulary size: {len(train_vocab):,} types\\\")\\n\",\n    \"print(f\\\"\\\\nValidation Set:\\\")\\n\",\n    \"print(f\\\"  Vocabulary size: {len(val_vocab):,} types\\\")\\n\",\n    \"print(f\\\"  OOV types: {len(val_oov):,} ({val_oov_rate:.2f}%)\\\")\\n\",\n    \"print(f\\\"  OOV token rate: {val_oov_token_rate:.2f}%\\\")\\n\",\n    \"print(f\\\"\\\\nTest Set:\\\")\\n\",\n    \"print(f\\\"  Vocabulary size: {len(test_vocab):,} types\\\")\\n\",\n    \"print(f\\\"  OOV types: {len(test_oov):,} ({test_oov_rate:.2f}%)\\\")\\n\",\n    \"print(f\\\"  OOV token rate: {test_oov_token_rate:.2f}%\\\")\\n\",\n    \"\\n\",\n    \"# Visualize\\n\",\n    \"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\",\n    \"\\n\",\n    \"# Type-level OOV\\n\",\n    \"splits = ['Val', 'Test']\\n\",\n    \"type_oov = [val_oov_rate, test_oov_rate]\\n\",\n    \"\\n\",\n    \"bars = axes[0].bar(splits, type_oov, color=['steelblue', 'coral'], edgecolor='black')\\n\",\n    \"axes[0].set_ylabel('OOV Rate (%)')\\n\",\n    \"axes[0].set_title('Type-Level OOV Rate', fontweight='bold')\\n\",\n    \"axes[0].grid(True, alpha=0.3, axis='y')\\n\",\n    \"\\n\",\n    \"for bar in bars:\\n\",\n    \"    height = bar.get_height()\\n\",\n    \"    axes[0].text(bar.get_x() + bar.get_width()/2., height,\\n\",\n    \"                f'{height:.2f}%', ha='center', va='bottom')\\n\",\n    \"\\n\",\n    \"# Token-level OOV\\n\",\n    \"token_oov = [val_oov_token_rate, test_oov_token_rate]\\n\",\n    \"\\n\",\n    \"bars = axes[1].bar(splits, token_oov, color=['steelblue', 'coral'], edgecolor='black')\\n\",\n    \"axes[1].set_ylabel('OOV Rate (%)')\\n\",\n    \"axes[1].set_title('Token-Level OOV Rate', fontweight='bold')\\n\",\n    \"axes[1].grid(True, alpha=0.3, axis='y')\\n\",\n    \"\\n\",\n    \"for bar in bars:\\n\",\n    \"    height = bar.get_height()\\n\",\n    \"    axes[1].text(bar.get_x() + bar.get_width()/2., height,\\n\",\n    \"                f'{height:.2f}%', ha='center', va='bottom')\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.savefig(FIGURES_DIR / 'oov_rates.png', dpi=300, bbox_inches='tight')\\n\",\n    \"plt.show()\"\n])\n\nprint(\"Sections 3-5 complete, continuing with sections 6-14...\")\n\n# Add all cells to notebook\nnotebook['cells'].extend(all_new_cells)\n\n# Save updated notebook\nwith open(notebook_path, 'w', encoding='utf-8') as f:\n    json.dump(notebook, f, ensure_ascii=False, indent=1)\n\nprint(f\"\\n‚úÖ Notebook updated with comprehensive EDA sections\")\nprint(f\"üìä Total cells: {len(notebook['cells'])}\")\nprint(f\"\\nüéØ Completed sections: 0 (Setup), 1 (Loading), 2 (Statistics), 3 (Distributions), 4 (Characters), 5 (Words)\")\nprint(f\"‚è≠Ô∏è  Remaining to add: Morphology, Syntax, POS, NER, Linguistic Phenomena, Tokenization, Quality, Cross-Source, Summary\")\n